{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layer_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "o88-UdqzVxuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1dd18bee-f048-4b1c-bd5e-0ec14beb0a7b"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, Activation\n",
        "from keras.layers import Embedding, Input, Dense, Dropout, Lambda, MaxPooling1D\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "p0NYgkTMV9HY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "9d9db8d6-b221-4c0b-e50c-20b4d858ef9d"
      },
      "cell_type": "code",
      "source": [
        "#len(ALPHABET)=68\n",
        "ALPHABET = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’\"/|_#$%ˆ&*˜‘+=<>()[]{} '\n",
        "FEATURE_LEN = 140 #maxlen\n",
        "path = '../data/'\n",
        "file=pd.read_csv('training.csv',encoding ='latin-1',header=None)\n",
        "file.info()\n",
        "file.drop([1,2,3,4],axis=1,inplace=True)\n",
        "file.columns='Sentiment','Text'\n",
        "TRAIN_DATA_FILE=file[:1520000]\n",
        "TRAIN_DATA_FILE = TRAIN_DATA_FILE.sample(frac=0.30, random_state=99)\n",
        "TEST_DATA_FILE=file[1520000:]\n",
        "target=TEST_DATA_FILE['Sentiment']\n",
        "TEST_DATA_FILE.drop(['Sentiment'],axis=1,inplace=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            "0    1600000 non-null int64\n",
            "1    1600000 non-null int64\n",
            "2    1600000 non-null object\n",
            "3    1600000 non-null object\n",
            "4    1600000 non-null object\n",
            "5    1600000 non-null object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qosl-Ah_WRJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e72f6816-7e5a-4cca-c554-e0925f04e6f2"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;36mdatalab\u001b[0m@  \u001b[01;34msample_data\u001b[0m/  training.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5XmCLZx_WDER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6068e3c7-6288-4dd9-81e9-788013876245"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DATA_FILE.info()\n",
        "TEST_DATA_FILE.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 456000 entries, 1041223 to 44300\n",
            "Data columns (total 2 columns):\n",
            "Sentiment    456000 non-null int64\n",
            "Text         456000 non-null object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 10.4+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 80000 entries, 1520000 to 1599999\n",
            "Data columns (total 1 columns):\n",
            "Text    80000 non-null object\n",
            "dtypes: object(1)\n",
            "memory usage: 625.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WYwrjAc7Y0R7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_char_dict():\n",
        "    char_dict={}\n",
        "    for i,c in enumerate(ALPHABET):\n",
        "        char_dict[c]=i+1\n",
        "    return char_dict\n",
        "\n",
        "def char2vec(text, max_length=FEATURE_LEN):\n",
        "    char_dict = get_char_dict()\n",
        "    data=np.zeros(max_length)\n",
        "    \n",
        "    for i in range(0, len(text)):\n",
        "        if i >= max_length:\n",
        "            return data\n",
        "        \n",
        "        elif text[i] in char_dict:\n",
        "            data[i] = char_dict[text[i]]\n",
        "        \n",
        "        else:\n",
        "            data[i]=68\n",
        "    return data\n",
        "    \n",
        "\n",
        "def conv_shape(conv):\n",
        "    return conv.get_shape().as_list()[1:]\n",
        "\n",
        "replace_ip=re.compile(r'([0-9]+)(?:\\.[0-9]+){3}',)\n",
        "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    \n",
        "    #Replace IP address\n",
        "    text=replace_ip.sub('',text)\n",
        "    \n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8tve3jRY4y2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = TRAIN_DATA_FILE\n",
        "test_df = TEST_DATA_FILE\n",
        "\n",
        "list_sentences_train = train_df['Text'].values\n",
        "y = train_df['Sentiment'].values\n",
        "for i in range(len(y)):\n",
        "    if(y[i] in [0,1]):\n",
        "        y[i]=0\n",
        "    elif(y[i] in [2,3]):\n",
        "        y[i]=1\n",
        "    else:\n",
        "        y[i]=2\n",
        "list_sentences_test = test_df['Text'].values\n",
        "data=[]\n",
        "for text in list_sentences_train:\n",
        "    data.append(char2vec(text_to_wordlist(text)))\n",
        "data=np.array(data)\n",
        "\n",
        "test_data = []\n",
        "for text in list_sentences_test:\n",
        "    test_data.append(char2vec(text_to_wordlist(text)))\n",
        "test_data=np.array(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kNqO2rxqY7fP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "59356bad-ed14-4b24-a1ea-63a50c6c5fc1"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "K7_i52FJZL3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d87194db-156d-4265-fda4-5d24c2ef5227"
      },
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 145)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "kNjT8bsEbjby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ConvolutionalBlock(input_shape, num_filters):\n",
        "    model=Sequential()\n",
        "\n",
        "    #1st conv layer\n",
        "    model.add(Conv1D(filters = num_filters, kernel_size = 3, strides = 1, padding = \"same\", input_shape = input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    #2nd conv layer\n",
        "    model.add(Conv1D(filters = num_filters, kernel_size = 3, strides = 1, padding = \"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-pHNP_VbmK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vdcnn_model(num_filters, num_classes, sequence_max_length, num_chars, embedding_size, top_k, learning_rate=0.001):\n",
        "    \n",
        "    inputs=Input(shape=(sequence_max_length, ), dtype='int32', name='input')\n",
        "    \n",
        "    embedded_seq = Embedding(num_chars, embedding_size, input_length=sequence_max_length)(inputs)\n",
        "    embedded_seq = BatchNormalization()(embedded_seq)\n",
        "    #1st Layer\n",
        "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_seq)\n",
        "    \n",
        "    #ConvBlocks\n",
        "    for i in range(len(num_filters)):\n",
        "        conv = ConvolutionalBlock(conv_shape(conv), num_filters[i])(conv)\n",
        "        conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
        "        \n",
        "    def _top_k(x):\n",
        "        x = tf.transpose(x, [0, 2, 1])\n",
        "        k_max = tf.nn.top_k(x, k=top_k)\n",
        "        return tf.reshape(k_max[0], (-1, num_filters[-1] * top_k))\n",
        "    \n",
        "    k_max = Lambda(_top_k, output_shape=(num_filters[-1] * top_k,))(conv)\n",
        "    \n",
        "    #fully connected layers\n",
        "    fc1=Dense(512, activation='relu', kernel_initializer='he_normal')(k_max)\n",
        "    fc1=Dropout(0.3)(fc1)\n",
        "    fc2=Dense(512, activation='relu', kernel_initializer='he_normal')(fc1)\n",
        "    fc2=Dropout(0.3)(fc2)\n",
        "    out=Dense(num_classes, activation='sigmoid')(fc2)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=out)\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z6-U6uUobqJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "20e2dfd0-c3b4-48bb-9cf8-aebbbf4c4c48"
      },
      "cell_type": "code",
      "source": [
        "num_filters = [64, 128, 256, 512]\n",
        "model=vdcnn_model(num_filters=num_filters, num_classes=6,num_chars=69, sequence_max_length=FEATURE_LEN,embedding_size=16,top_k=3)\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 145)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 145, 16)           1104      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 145, 16)           64        \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 73, 64)            3136      \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 73, 64)            25216     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 37, 64)            0         \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 37, 128)           75008     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 19, 128)           0         \n",
            "_________________________________________________________________\n",
            "sequential_3 (Sequential)    (None, 19, 256)           297472    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 10, 512)           1184768   \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               786944    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 2,639,446\n",
            "Trainable params: 2,635,574\n",
            "Non-trainable params: 3,872\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p_tsqupSbsfM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y= to_categorical(y, num_classes=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--G9KeSMbxDO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "def limit_mem():\n",
        "    import keras.backend as K\n",
        "    K.get_session().close()\n",
        "    cfg = K.tf.ConfigProto()\n",
        "    cfg.gpu_options.allow_growth = True\n",
        "    K.set_session(K.tf.Session(config=cfg))\n",
        "    print('gpu memory cleaned')\n",
        "    \n",
        "def preds(k):\n",
        "    from datetime import datetime\n",
        "    y_temp = np.zeros((len(test_data), 3))\n",
        "    y_pred = np.zeros((len(test_data), 3))\n",
        "    i=0;\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=k, random_state=2)\n",
        "\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        limit_mem()\n",
        "        start=datetime.now()\n",
        "        print('fold====================>>>>>>>>>>',i+1)\n",
        "        X_train, X_test = data[train_index], data[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        #np_utils.to_categorical(y_train, nb_classes=6)\n",
        "\n",
        "        model = None\n",
        "        num_filters = [64, 128, 256, 512]\n",
        "        model=vdcnn_model(num_filters=num_filters, num_classes=3,num_chars=69, sequence_max_length=FEATURE_LEN,embedding_size=16,top_k=3)\n",
        "\n",
        "        early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
        "        bst_model_path = 'cv10_best_weights'+str(i+1) + '.h5'\n",
        "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "        hist = model.fit(X_train, y_train, \\\n",
        "                validation_data=(X_test, y_test), \\\n",
        "                epochs=200, batch_size=256,callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "        bst_val_score = min(hist.history['val_loss'])\n",
        "        print('bst_val_score',bst_val_score)\n",
        "\n",
        "        model.load_weights(bst_model_path)\n",
        "        #model.fit(data, y,epochs=2, batch_size=256, shuffle=True,)\n",
        "        \n",
        "        y_temp = model.predict([test_data], batch_size=256, verbose=1)\n",
        "        y_pred+=y_temp\n",
        "        end=datetime.now()\n",
        "        print(\" \")\n",
        "        print('time taken for this fold', end-start)\n",
        "        i+=1\n",
        "    y_test_pred=y_pred/k\n",
        "    return y_test_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYEzBfozb0nF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5423
        },
        "outputId": "0ff2e260-d0ad-4bf2-9603-7b041bad9999"
      },
      "cell_type": "code",
      "source": [
        "y_test=preds(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 1\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 196s 478us/step - loss: 0.4116 - acc: 0.7806 - val_loss: 0.4460 - val_acc: 0.7879\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 188s 458us/step - loss: 0.3488 - acc: 0.8243 - val_loss: 0.3436 - val_acc: 0.8257\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 187s 456us/step - loss: 0.3348 - acc: 0.8346 - val_loss: 0.3442 - val_acc: 0.8270\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 190s 463us/step - loss: 0.3265 - acc: 0.8405 - val_loss: 0.4777 - val_acc: 0.7592\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.3205 - acc: 0.8445 - val_loss: 0.3317 - val_acc: 0.8369\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 184s 448us/step - loss: 0.3157 - acc: 0.8477 - val_loss: 0.3644 - val_acc: 0.8281\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 184s 448us/step - loss: 0.3116 - acc: 0.8502 - val_loss: 0.3321 - val_acc: 0.8325\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 184s 449us/step - loss: 0.3075 - acc: 0.8533 - val_loss: 0.3319 - val_acc: 0.8407\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 181s 441us/step - loss: 0.3043 - acc: 0.8554 - val_loss: 0.3419 - val_acc: 0.8287\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 181s 440us/step - loss: 0.3008 - acc: 0.8574 - val_loss: 0.3307 - val_acc: 0.8414\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.2975 - acc: 0.8593 - val_loss: 0.5173 - val_acc: 0.8185\n",
            "Epoch 12/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.2940 - acc: 0.8617 - val_loss: 0.3759 - val_acc: 0.8035\n",
            "Epoch 13/200\n",
            "410400/410400 [==============================] - 182s 443us/step - loss: 0.2913 - acc: 0.8628 - val_loss: 0.3330 - val_acc: 0.8340\n",
            "Epoch 14/200\n",
            "410400/410400 [==============================] - 182s 444us/step - loss: 0.2885 - acc: 0.8648 - val_loss: 0.3651 - val_acc: 0.8179\n",
            "Epoch 15/200\n",
            "410400/410400 [==============================] - 182s 444us/step - loss: 0.2853 - acc: 0.8667 - val_loss: 0.3434 - val_acc: 0.8336\n",
            "bst_val_score 0.3307097554206848\n",
            "80000/80000 [==============================] - 15s 194us/step\n",
            " \n",
            "time taken for this fold 0:46:43.113856\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 2\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 198s 481us/step - loss: 0.4107 - acc: 0.7770 - val_loss: 0.4433 - val_acc: 0.7481\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 198s 483us/step - loss: 0.3499 - acc: 0.8240 - val_loss: 0.3915 - val_acc: 0.7739\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 199s 485us/step - loss: 0.3356 - acc: 0.8342 - val_loss: 0.3421 - val_acc: 0.8306\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 192s 469us/step - loss: 0.3271 - acc: 0.8400 - val_loss: 0.3970 - val_acc: 0.8105\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 191s 465us/step - loss: 0.3211 - acc: 0.8443 - val_loss: 0.3360 - val_acc: 0.8334\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 192s 467us/step - loss: 0.3170 - acc: 0.8473 - val_loss: 0.3417 - val_acc: 0.8371\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 192s 468us/step - loss: 0.3127 - acc: 0.8500 - val_loss: 0.3426 - val_acc: 0.8293\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 190s 464us/step - loss: 0.3100 - acc: 0.8525 - val_loss: 0.3414 - val_acc: 0.8363\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 191s 465us/step - loss: 0.3060 - acc: 0.8547 - val_loss: 0.3705 - val_acc: 0.8075\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 192s 467us/step - loss: 0.3034 - acc: 0.8567 - val_loss: 0.4525 - val_acc: 0.8254\n",
            "bst_val_score 0.33599535377402057\n",
            "80000/80000 [==============================] - 16s 205us/step\n",
            " \n",
            "time taken for this fold 0:32:39.373604\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 3\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 200s 486us/step - loss: 0.4118 - acc: 0.7753 - val_loss: 0.3784 - val_acc: 0.8011\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 192s 469us/step - loss: 0.3498 - acc: 0.8241 - val_loss: 0.4597 - val_acc: 0.7864\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.3359 - acc: 0.8343 - val_loss: 0.3587 - val_acc: 0.8144\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 185s 450us/step - loss: 0.3288 - acc: 0.8396 - val_loss: 0.3372 - val_acc: 0.8332\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.3223 - acc: 0.8437 - val_loss: 0.4737 - val_acc: 0.7742\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.3185 - acc: 0.8464 - val_loss: 0.3360 - val_acc: 0.8374\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 184s 449us/step - loss: 0.3146 - acc: 0.8495 - val_loss: 0.3556 - val_acc: 0.8263\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 189s 460us/step - loss: 0.3112 - acc: 0.8514 - val_loss: 0.3432 - val_acc: 0.8272\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 188s 459us/step - loss: 0.3146 - acc: 0.8533 - val_loss: 0.3496 - val_acc: 0.8408\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 184s 449us/step - loss: 0.3098 - acc: 0.8553 - val_loss: 0.3524 - val_acc: 0.8264\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.3037 - acc: 0.8574 - val_loss: 0.3377 - val_acc: 0.8376\n",
            "bst_val_score 0.3359971465980797\n",
            "80000/80000 [==============================] - 16s 198us/step\n",
            " \n",
            "time taken for this fold 0:34:52.242006\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 4\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 195s 474us/step - loss: 0.4095 - acc: 0.7763 - val_loss: 0.3749 - val_acc: 0.8027\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 188s 459us/step - loss: 0.3515 - acc: 0.8228 - val_loss: 0.3663 - val_acc: 0.8205\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 192s 468us/step - loss: 0.3378 - acc: 0.8329 - val_loss: 0.3439 - val_acc: 0.8273\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 188s 459us/step - loss: 0.3298 - acc: 0.8389 - val_loss: 0.3449 - val_acc: 0.8267\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 187s 457us/step - loss: 0.3245 - acc: 0.8427 - val_loss: 0.3457 - val_acc: 0.8349\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 189s 459us/step - loss: 0.3214 - acc: 0.8459 - val_loss: 0.3313 - val_acc: 0.8381\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 187s 454us/step - loss: 0.3200 - acc: 0.8486 - val_loss: 0.3256 - val_acc: 0.8411\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 187s 456us/step - loss: 0.3141 - acc: 0.8507 - val_loss: 0.3274 - val_acc: 0.8407\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 190s 464us/step - loss: 0.3152 - acc: 0.8523 - val_loss: 0.3305 - val_acc: 0.8366\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.3103 - acc: 0.8542 - val_loss: 0.3588 - val_acc: 0.8306\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 186s 452us/step - loss: 0.3088 - acc: 0.8559 - val_loss: 0.3320 - val_acc: 0.8420\n",
            "Epoch 12/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.3063 - acc: 0.8572 - val_loss: 0.3319 - val_acc: 0.8401\n",
            "bst_val_score 0.3255932216058698\n",
            "80000/80000 [==============================] - 16s 202us/step\n",
            " \n",
            "time taken for this fold 0:38:07.958647\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 5\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 195s 475us/step - loss: 0.4141 - acc: 0.7747 - val_loss: 0.3778 - val_acc: 0.8144\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 188s 459us/step - loss: 0.3500 - acc: 0.8236 - val_loss: 0.3824 - val_acc: 0.8110\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 190s 462us/step - loss: 0.3363 - acc: 0.8339 - val_loss: 0.3448 - val_acc: 0.8305\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 187s 454us/step - loss: 0.3277 - acc: 0.8397 - val_loss: 0.3566 - val_acc: 0.8235\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.3228 - acc: 0.8440 - val_loss: 0.3549 - val_acc: 0.8159\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.3176 - acc: 0.8475 - val_loss: 0.3518 - val_acc: 0.8298\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 186s 452us/step - loss: 0.3149 - acc: 0.8500 - val_loss: 0.3631 - val_acc: 0.8225\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 184s 448us/step - loss: 0.3112 - acc: 0.8516 - val_loss: 0.3246 - val_acc: 0.8384\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 187s 457us/step - loss: 0.3067 - acc: 0.8542 - val_loss: 0.3982 - val_acc: 0.7992\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 191s 466us/step - loss: 0.3067 - acc: 0.8565 - val_loss: 0.3256 - val_acc: 0.8434\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 187s 456us/step - loss: 0.3000 - acc: 0.8582 - val_loss: 0.3426 - val_acc: 0.8361\n",
            "Epoch 12/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.3026 - acc: 0.8601 - val_loss: 0.3500 - val_acc: 0.8310\n",
            "Epoch 13/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.2973 - acc: 0.8617 - val_loss: 0.3307 - val_acc: 0.8409\n",
            "bst_val_score 0.3246078070423059\n",
            "80000/80000 [==============================] - 17s 207us/step\n",
            " \n",
            "time taken for this fold 0:41:03.808550\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 6\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 198s 482us/step - loss: 0.4167 - acc: 0.7698 - val_loss: 0.4578 - val_acc: 0.7908\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 188s 458us/step - loss: 0.3500 - acc: 0.8239 - val_loss: 0.3565 - val_acc: 0.8220\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 187s 456us/step - loss: 0.3367 - acc: 0.8336 - val_loss: 1.2156 - val_acc: 0.7596\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 191s 465us/step - loss: 0.3296 - acc: 0.8391 - val_loss: 0.3506 - val_acc: 0.8299\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 190s 464us/step - loss: 0.3242 - acc: 0.8436 - val_loss: 1.6737 - val_acc: 0.7712\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.3203 - acc: 0.8465 - val_loss: 0.3450 - val_acc: 0.8357\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.3152 - acc: 0.8493 - val_loss: 0.3311 - val_acc: 0.8375\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.3134 - acc: 0.8516 - val_loss: 0.3361 - val_acc: 0.8383\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.3112 - acc: 0.8528 - val_loss: 0.3386 - val_acc: 0.8314\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.3084 - acc: 0.8555 - val_loss: 0.4054 - val_acc: 0.8093\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.3083 - acc: 0.8570 - val_loss: 0.3536 - val_acc: 0.8335\n",
            "Epoch 12/200\n",
            "410400/410400 [==============================] - 191s 464us/step - loss: 0.3090 - acc: 0.8578 - val_loss: 2.3201 - val_acc: 0.8005\n",
            "bst_val_score 0.33107336470955295\n",
            "80000/80000 [==============================] - 18s 220us/step\n",
            " \n",
            "time taken for this fold 0:38:09.211560\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 7\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 199s 485us/step - loss: 0.4250 - acc: 0.7705 - val_loss: 0.4342 - val_acc: 0.7918\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.3529 - acc: 0.8221 - val_loss: 0.3535 - val_acc: 0.8263\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.3387 - acc: 0.8327 - val_loss: 0.3427 - val_acc: 0.8315\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.3297 - acc: 0.8388 - val_loss: 0.3420 - val_acc: 0.8360\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.3232 - acc: 0.8437 - val_loss: 0.3587 - val_acc: 0.8315\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 185s 452us/step - loss: 0.3184 - acc: 0.8474 - val_loss: 0.3421 - val_acc: 0.8262\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 187s 457us/step - loss: 0.3133 - acc: 0.8505 - val_loss: 0.3396 - val_acc: 0.8345\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 190s 462us/step - loss: 0.3100 - acc: 0.8527 - val_loss: 0.3440 - val_acc: 0.8234\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 187s 456us/step - loss: 0.3056 - acc: 0.8549 - val_loss: 0.3315 - val_acc: 0.8390\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 182s 445us/step - loss: 0.3017 - acc: 0.8579 - val_loss: 0.3330 - val_acc: 0.8423\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 184s 449us/step - loss: 0.2975 - acc: 0.8596 - val_loss: 0.3251 - val_acc: 0.8415\n",
            "Epoch 12/200\n",
            "410400/410400 [==============================] - 186s 453us/step - loss: 0.2941 - acc: 0.8619 - val_loss: 0.3295 - val_acc: 0.8390\n",
            "Epoch 13/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.2929 - acc: 0.8643 - val_loss: 0.3279 - val_acc: 0.8415\n",
            "Epoch 14/200\n",
            "410400/410400 [==============================] - 185s 450us/step - loss: 0.2872 - acc: 0.8662 - val_loss: 0.3813 - val_acc: 0.8364\n",
            "Epoch 15/200\n",
            "410400/410400 [==============================] - 188s 458us/step - loss: 0.2833 - acc: 0.8683 - val_loss: 0.3358 - val_acc: 0.8335\n",
            "Epoch 16/200\n",
            "410400/410400 [==============================] - 188s 459us/step - loss: 0.2815 - acc: 0.8707 - val_loss: 0.3645 - val_acc: 0.8361\n",
            "bst_val_score 0.32510320895596556\n",
            "80000/80000 [==============================] - 18s 220us/step\n",
            " \n",
            "time taken for this fold 0:50:24.811347\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 8\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 196s 477us/step - loss: 0.4120 - acc: 0.7843 - val_loss: 0.4527 - val_acc: 0.7684\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 191s 465us/step - loss: 0.3506 - acc: 0.8242 - val_loss: 0.4212 - val_acc: 0.7717\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 190s 462us/step - loss: 0.3372 - acc: 0.8344 - val_loss: 0.3458 - val_acc: 0.8258\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 185s 451us/step - loss: 0.3283 - acc: 0.8404 - val_loss: 0.3387 - val_acc: 0.8323\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 192s 468us/step - loss: 0.3220 - acc: 0.8443 - val_loss: 0.3517 - val_acc: 0.8204\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 194s 473us/step - loss: 0.3178 - acc: 0.8475 - val_loss: 0.3270 - val_acc: 0.8404\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 190s 463us/step - loss: 0.3149 - acc: 0.8501 - val_loss: 0.4182 - val_acc: 0.7973\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 190s 462us/step - loss: 0.3105 - acc: 0.8526 - val_loss: 0.3355 - val_acc: 0.8375\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 192s 467us/step - loss: 0.3071 - acc: 0.8549 - val_loss: 0.3372 - val_acc: 0.8355\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 191s 464us/step - loss: 0.3028 - acc: 0.8564 - val_loss: 0.3435 - val_acc: 0.8312\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 189s 460us/step - loss: 0.3001 - acc: 0.8585 - val_loss: 0.3334 - val_acc: 0.8416\n",
            "bst_val_score 0.32701541365238657\n",
            "80000/80000 [==============================] - 17s 213us/step\n",
            " \n",
            "time taken for this fold 0:35:28.696821\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 9\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 201s 491us/step - loss: 0.4132 - acc: 0.7766 - val_loss: 0.3697 - val_acc: 0.8136\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 195s 474us/step - loss: 0.3495 - acc: 0.8239 - val_loss: 0.3499 - val_acc: 0.8280\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 190s 464us/step - loss: 0.3354 - acc: 0.8343 - val_loss: 0.3545 - val_acc: 0.8257\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 190s 462us/step - loss: 0.3273 - acc: 0.8402 - val_loss: 0.4102 - val_acc: 0.7687\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 188s 458us/step - loss: 0.3210 - acc: 0.8439 - val_loss: 0.3438 - val_acc: 0.8321\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 189s 461us/step - loss: 0.3163 - acc: 0.8476 - val_loss: 0.3688 - val_acc: 0.8185\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 188s 458us/step - loss: 0.3116 - acc: 0.8505 - val_loss: 0.3410 - val_acc: 0.8349\n",
            "Epoch 8/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.3078 - acc: 0.8531 - val_loss: 0.3596 - val_acc: 0.8260\n",
            "Epoch 9/200\n",
            "410400/410400 [==============================] - 189s 461us/step - loss: 0.3047 - acc: 0.8551 - val_loss: 0.3355 - val_acc: 0.8355\n",
            "Epoch 10/200\n",
            "410400/410400 [==============================] - 191s 464us/step - loss: 0.3014 - acc: 0.8566 - val_loss: 0.3899 - val_acc: 0.8302\n",
            "Epoch 11/200\n",
            "410400/410400 [==============================] - 189s 460us/step - loss: 0.2979 - acc: 0.8586 - val_loss: 0.3947 - val_acc: 0.8045\n",
            "Epoch 12/200\n",
            "410400/410400 [==============================] - 187s 457us/step - loss: 0.2953 - acc: 0.8604 - val_loss: 0.3332 - val_acc: 0.8388\n",
            "Epoch 13/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.2922 - acc: 0.8624 - val_loss: 0.3349 - val_acc: 0.8420\n",
            "Epoch 14/200\n",
            "410400/410400 [==============================] - 187s 455us/step - loss: 0.2891 - acc: 0.8642 - val_loss: 0.3445 - val_acc: 0.8329\n",
            "Epoch 15/200\n",
            "410400/410400 [==============================] - 183s 447us/step - loss: 0.2861 - acc: 0.8665 - val_loss: 0.3395 - val_acc: 0.8406\n",
            "Epoch 16/200\n",
            "410400/410400 [==============================] - 177s 432us/step - loss: 0.2826 - acc: 0.8681 - val_loss: 0.3389 - val_acc: 0.8388\n",
            "Epoch 17/200\n",
            "410400/410400 [==============================] - 180s 439us/step - loss: 0.2795 - acc: 0.8702 - val_loss: 0.4282 - val_acc: 0.8264\n",
            "bst_val_score 0.3332276157328957\n",
            "80000/80000 [==============================] - 18s 222us/step\n",
            " \n",
            "time taken for this fold 0:53:49.816436\n",
            "gpu memory cleaned\n",
            "fold====================>>>>>>>>>> 10\n",
            "Train on 410400 samples, validate on 45600 samples\n",
            "Epoch 1/200\n",
            "410400/410400 [==============================] - 203s 496us/step - loss: 0.4144 - acc: 0.7847 - val_loss: 0.3938 - val_acc: 0.7935\n",
            "Epoch 2/200\n",
            "410400/410400 [==============================] - 191s 465us/step - loss: 0.3502 - acc: 0.8246 - val_loss: 0.3478 - val_acc: 0.8267\n",
            "Epoch 3/200\n",
            "410400/410400 [==============================] - 188s 458us/step - loss: 0.3362 - acc: 0.8347 - val_loss: 0.4768 - val_acc: 0.7065\n",
            "Epoch 4/200\n",
            "410400/410400 [==============================] - 188s 457us/step - loss: 0.3278 - acc: 0.8403 - val_loss: 0.3636 - val_acc: 0.8076\n",
            "Epoch 5/200\n",
            "410400/410400 [==============================] - 189s 460us/step - loss: 0.3216 - acc: 0.8442 - val_loss: 0.3558 - val_acc: 0.8119\n",
            "Epoch 6/200\n",
            "410400/410400 [==============================] - 186s 454us/step - loss: 0.3164 - acc: 0.8474 - val_loss: 0.3523 - val_acc: 0.8268\n",
            "Epoch 7/200\n",
            "410400/410400 [==============================] - 184s 447us/step - loss: 0.3127 - acc: 0.8504 - val_loss: 0.3562 - val_acc: 0.8199\n",
            "bst_val_score 0.3478155595587011\n",
            "80000/80000 [==============================] - 17s 214us/step\n",
            " \n",
            "time taken for this fold 0:22:41.378415\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}